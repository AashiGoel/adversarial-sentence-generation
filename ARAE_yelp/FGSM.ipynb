{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify\n",
    "from models import Seq2Seq2Decoder, Seq2Seq, MLP_D, MLP_G, MLP_Classify\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = \"./data\"\n",
    "OUTF = \"yelp_example\"\n",
    "MODELF = \"model_output\"\n",
    "VOCAB_SIZE = 8000\n",
    "MAXLEN = 30\n",
    "LOWERCASE = True\n",
    "EMSIZE = 300\n",
    "NHIDDEN = 512\n",
    "BATCH_SIZE = 64\n",
    "NLAYERS = 1\n",
    "NOISE_R = 0.1\n",
    "DROPOUT = 0.0\n",
    "HIDDEN_INIT = True ## ? Is this the correct default value?\n",
    "Z_SIZE = 32\n",
    "#Changed the architecture: Removed batch norm layer and made it dense\n",
    "#Issue: Because we're handling data for +ve and -ve classes separately, the batch norm is skewed towards the latter dataset\n",
    "#While evaluating the former dataset, these metrics are wrong\n",
    "#Alt Solution: Run a few forward passes without gradients enabled in model.train() mode, so the batch norm is recitified\n",
    "ARCH_CLASSIFY = '128-128-128' \n",
    "ARCH_D = '128-128'\n",
    "ARCH_G = '128-128'\n",
    "# learning rates\n",
    "LR_AE = 1\n",
    "LR_GAN_G = 1e-04\n",
    "LR_GAN_D = 1e-04\n",
    "LR_CLASSIFY = 1e-04\n",
    "BETA1 = 0.4\n",
    "CUDA = True\n",
    "SEED = 1111\n",
    "LAMBDA_CLASS = 1\n",
    "CLIP = 1\n",
    "TEMP = 1\n",
    "LOG_INTERVAL = 200\n",
    "NITERS_GAN_SCHEDULE = ''\n",
    "GRAD_LAMBDA = 0.01\n",
    "GAN_GP_LAMBDA = 0.1\n",
    "EPOCHS = 0\n",
    "NITERS_AE = 1\n",
    "NITERS_GAN_G = 1\n",
    "NITERS_GAN_D = 5\n",
    "NITERS_GAN_AE = 1\n",
    "NOISE_ANNEAL = 0.9995\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# make output directory if it doesn't already exist\n",
    "if os.path.isdir(OUTF):\n",
    "    shutil.rmtree(OUTF)\n",
    "os.makedirs(OUTF)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WARNING: You have a CUDA device, \"\n",
    "              \"so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab 8092; Pruned to 8004\n",
      "Number of sentences dropped from ./data/valid1.txt: 0 out of 38205 total\n",
      "Number of sentences dropped from ./data/valid2.txt: 3 out of 25278 total\n",
      "Number of sentences dropped from ./data/test1.txt: 0 out of 76392 total\n",
      "Number of sentences dropped from ./data/test0.txt: 5 out of 50278 total\n",
      "Number of sentences dropped from ./data/train1.txt: 1 out of 267314 total\n",
      "Number of sentences dropped from ./data/train2.txt: 4 out of 176787 total\n",
      "Vocabulary Size: 8004\n",
      "382 batches\n",
      "252 batches\n",
      "763 batches\n",
      "502 batches\n",
      "4176 batches\n",
      "2762 batches\n",
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "label_ids = {\"pos\": 1, \"neg\": 0}\n",
    "id2label = {1:\"pos\", 0:\"neg\"}\n",
    "\n",
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [(os.path.join(DATA_PATH, \"valid1.txt\"), \"valid1\", False),\n",
    "             (os.path.join(DATA_PATH, \"valid2.txt\"), \"valid2\", False),\n",
    "             (os.path.join(DATA_PATH, \"test1.txt\"), \"test1\", False),\n",
    "             (os.path.join(DATA_PATH, \"test0.txt\"), \"test2\", False),\n",
    "             (os.path.join(DATA_PATH, \"train1.txt\"), \"train1\", True),\n",
    "             (os.path.join(DATA_PATH, \"train2.txt\"), \"train2\", True)]\n",
    "vocabdict = None\n",
    "corpus = Corpus(datafiles,\n",
    "                maxlen=MAXLEN,\n",
    "                vocab_size=VOCAB_SIZE,\n",
    "                lowercase=LOWERCASE,\n",
    "                vocab=vocabdict)\n",
    "\n",
    "# dumping vocabulary\n",
    "with open('{}/vocab.json'.format(OUTF), 'w') as f:\n",
    "    json.dump(corpus.dictionary.word2idx, f)\n",
    "\n",
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))\n",
    "\n",
    "eval_batch_size = 100\n",
    "val_data = batchify(corpus.data['valid1'], eval_batch_size, shuffle=False)\n",
    "val2_data = batchify(corpus.data['valid2'], eval_batch_size, shuffle=False)\n",
    "test1_data = batchify(corpus.data['test1'], eval_batch_size, shuffle=False)\n",
    "test2_data = batchify(corpus.data['test2'], eval_batch_size, shuffle=False)\n",
    "train1_data = batchify(corpus.data['train1'], BATCH_SIZE, shuffle=True)\n",
    "train2_data = batchify(corpus.data['train2'], BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq2Decoder(\n",
      "  (embedding): Embedding(8004, 300)\n",
      "  (embedding_decoder1): Embedding(8004, 300)\n",
      "  (embedding_decoder2): Embedding(8004, 300)\n",
      "  (encoder): LSTM(300, 512, batch_first=True)\n",
      "  (decoder1): LSTM(812, 512, batch_first=True)\n",
      "  (decoder2): LSTM(812, 512, batch_first=True)\n",
      "  (linear): Linear(in_features=512, out_features=8004, bias=True)\n",
      ")\n",
      "MLP_G(\n",
      "  (layer1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer7): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "MLP_D(\n",
      "  (layer1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (activation1): LeakyReLU(negative_slope=0.2)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): LeakyReLU(negative_slope=0.2)\n",
      "  (layer6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "MLP_Classify(\n",
      "  (layer1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (activation3): ReLU()\n",
      "  (layer7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "autoencoder = Seq2Seq2Decoder(emsize=EMSIZE,\n",
    "                      nhidden=NHIDDEN,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=NLAYERS,\n",
    "                      noise_r=NOISE_R,\n",
    "                      hidden_init=HIDDEN_INIT,\n",
    "                      dropout=DROPOUT,\n",
    "                      gpu=CUDA)\n",
    "\n",
    "gan_gen = MLP_G(ninput=Z_SIZE, noutput=NHIDDEN, layers=ARCH_G)\n",
    "gan_disc = MLP_D(ninput=NHIDDEN, noutput=1, layers=ARCH_D)\n",
    "classifier = MLP_Classify(ninput=NHIDDEN, noutput=1, layers=ARCH_CLASSIFY)\n",
    "g_factor = None\n",
    "\n",
    "print(autoencoder)\n",
    "print(gan_gen)\n",
    "print(gan_disc)\n",
    "print(classifier)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=LR_AE)\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=LR_GAN_G,\n",
    "                             betas=(BETA1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=LR_GAN_D,\n",
    "                             betas=(BETA1, 0.999))\n",
    "#### classify\n",
    "optimizer_classify = optim.Adam(classifier.parameters(),\n",
    "                                lr=LR_CLASSIFY,\n",
    "                                betas=(BETA1, 0.999))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if CUDA:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    gan_gen = gan_gen.cuda()\n",
    "    gan_disc = gan_disc.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "    criterion_ce = criterion_ce.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from model_output\n",
      "Evaluating classifier on validation data\n",
      "Classifier on Data 1, Loss:0.17387842319583519 Accuracy:0.9248232841491699\n",
      "Classifier on Data 2, Loss:0.26193418606106506 Accuracy:0.8982877135276794\n",
      "Epsilon: 0.0\n",
      "Classifier on Data 1, Loss:0.17387842319583519 Accuracy:0.9248232841491699\n",
      "Classifier on Data 2, Loss:0.26193418606106506 Accuracy:0.8982877135276794\n",
      "Epsilon: 0.0005\n",
      "Classifier on Data 1, Loss:0.4073128544799935 Accuracy:0.8168018460273743\n",
      "Classifier on Data 2, Loss:0.6338521083631363 Accuracy:0.7281673550605774\n",
      "Epsilon: 0.001\n",
      "Classifier on Data 1, Loss:0.8295734343881844 Accuracy:0.64922696352005\n",
      "Classifier on Data 2, Loss:1.3197042420803313 Accuracy:0.4825897216796875\n",
      "Epsilon: 0.0015\n",
      "Classifier on Data 1, Loss:1.4723540454673267 Accuracy:0.45005276799201965\n",
      "Classifier on Data 2, Loss:2.32494501573631 Accuracy:0.25681260228157043\n",
      "Epsilon: 0.002\n",
      "Classifier on Data 1, Loss:2.295497343393328 Accuracy:0.2662908732891083\n",
      "Classifier on Data 2, Loss:3.5174518624149944 Accuracy:0.11774896830320358\n",
      "Epsilon: 0.0025\n",
      "Classifier on Data 1, Loss:3.18299449911905 Accuracy:0.1389777958393097\n",
      "Classifier on Data 2, Loss:4.748179786233788 Accuracy:0.0480080284178257\n",
      "Epsilon: 0.003\n",
      "Classifier on Data 1, Loss:4.011954615031751 Accuracy:0.06692009419202805\n",
      "Classifier on Data 2, Loss:5.951915010513063 Accuracy:0.01721120811998844\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "               \n",
    "def load_model():\n",
    "    print(\"Loading models from {}\".format(MODELF))\n",
    "    if os.path.exists('{}/autoencoder_model.pt'.format(MODELF)):\n",
    "        autoencoder.load_state_dict(torch.load('{}/autoencoder_model.pt'.format(MODELF)))\n",
    "    if os.path.exists('{}/gan_gen_model.pt'.format(MODELF)):    \n",
    "        gan_gen.load_state_dict(torch.load('{}/gan_gen_model.pt'.format(MODELF)))\n",
    "    if os.path.exists('{}/gan_disc_model.pt'.format(MODELF)):    \n",
    "        gan_disc.load_state_dict(torch.load('{}/gan_disc_model.pt'.format(MODELF)))\n",
    "    if os.path.exists('{}/classifier_model.pt'.format(MODELF)):            \n",
    "        classifier.load_state_dict(torch.load('{}/classifier_model.pt'.format(MODELF)))\n",
    "\n",
    "def fgsm_attack(sentence_embedding, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_embedding = sentence_embedding + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    #is this required?\n",
    "    #perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_embedding\n",
    "\n",
    "def pgd_attack():\n",
    "    pass\n",
    "\n",
    "def evaluate_example(whichclass, data):\n",
    "    pass\n",
    "\n",
    "def evaluate_classifier(whichclass, data_source, epoch=0, perturb=False, epsilon=0.0):\n",
    "    classifier.eval()\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    #commenting as we need gradients\n",
    "    #with torch.no_grad():\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        source = to_gpu(CUDA, Variable(source))\n",
    "        labels = to_gpu(CUDA, Variable(torch.zeros(source.size(0)).fill_(whichclass-1)))\n",
    "        \n",
    "        code = autoencoder(0, source, lengths, noise=False, encode_only=True).detach()\n",
    "        code.requires_grad = True\n",
    "        scores = classifier(code)\n",
    "        classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "        classifier.zero_grad()\n",
    "        classify_loss.backward()\n",
    "        \n",
    "        code_grad = code.grad.data\n",
    "        perturbed_code = fgsm_attack(code, epsilon, code_grad)\n",
    "        scores = classifier(perturbed_code)\n",
    "        classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "        classify_loss = classify_loss.cpu().item()\n",
    "\n",
    "        pred = scores.data.round().squeeze(1)\n",
    "        accuracy = pred.eq(labels.data).float().mean()\n",
    "\n",
    "        total_loss += classify_loss\n",
    "        all_accuracies += accuracy\n",
    "        bcnt += 1\n",
    "    return total_loss/len(data_source), all_accuracies/bcnt\n",
    "\n",
    "#loading pre-trained weights\n",
    "\n",
    "load_model()\n",
    "\n",
    "#without attack\n",
    "print('Evaluating classifier on validation data')\n",
    "test_loss_classifier, test_accuracy_classifier = evaluate_classifier(1, test1_data)\n",
    "print('Classifier on Data 1, Loss:{} Accuracy:{}'.format(test_loss_classifier, test_accuracy_classifier))\n",
    "\n",
    "test_loss_classifier, test_accuracy_classifier = evaluate_classifier(2, test2_data)\n",
    "print('Classifier on Data 2, Loss:{} Accuracy:{}'.format(test_loss_classifier, test_accuracy_classifier))\n",
    "\n",
    "#fgsm \n",
    "epsilon = [0.000, 0.0005, 0.001, 0.0015, 0.002, 0.0025, 0.003]\n",
    "for e in epsilon:\n",
    "    print('Epsilon: {}'.format(e))\n",
    "    test_loss_classifier, test_accuracy_classifier = evaluate_classifier(1, test1_data, 0, True, e)\n",
    "    print('Classifier on Data 1, Loss:{} Accuracy:{}'.format(test_loss_classifier, test_accuracy_classifier))\n",
    "    \n",
    "    test_loss_classifier, test_accuracy_classifier = evaluate_classifier(2, test2_data, 0, True, e)\n",
    "    print('Classifier on Data 2, Loss:{} Accuracy:{}'.format(test_loss_classifier, test_accuracy_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
